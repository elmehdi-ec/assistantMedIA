import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# üîÅ Chargement une seule fois du mod√®le Flan-T5 Base
MODEL_NAME = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

# ‚öôÔ∏è D√©tection de l'environnement (GPU ou CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def generer_resume(symptomes: str, medecin_id: str, hf_token=None, mode_demo: bool = False) -> str:
    """
    G√©n√®re un r√©sum√© clinique en fran√ßais √† partir des sympt√¥mes du patient.
    Utilise le mod√®le local Flan-T5 Base. Le param√®tre hf_token est ignor√©.
    """
    if mode_demo:
        return f"(Mode d√©mo actif) {symptomes[:40]}..."

    # üß† Prompt simplifi√© pour am√©lioration de la compr√©hension par le mod√®le
    prompt = (
        f"Un patient de sexe inconnu nomm√© {medecin_id} pr√©sente les sympt√¥mes suivants : {symptomes}.\n"
        f"Quels sont l'hypoth√®se diagnostique, la conduite √† tenir et les examens compl√©mentaires recommand√©s ?"
    )

    try:
        # ‚õìÔ∏è Pr√©paration des entr√©es
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # üß™ G√©n√©ration du texte
        outputs = model.generate(**inputs, max_new_tokens=100)
        texte = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return texte.strip()

    except Exception as e:
        return f"‚ùå Erreur IA : {str(e)}"
